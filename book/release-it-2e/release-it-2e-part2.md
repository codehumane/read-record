# Case Study: Phenomenal Cosmic Powers, Itty-Bitty Living Space

새로운 사이트를 만들어서 운영하기 시작. 첫 추수 감사절은 잘 지나감. 다음 날인 블랙 프라이데이가 되었고, 시스템 모니터링 지표들을 살펴봄. 주문 수는 전날보다 늘었지만, 페이지 레이턴시는 여전히 250ms 이하를 유지하고 있었음. 그런데 어느 순간, 모든 DRP에 빨간색이 표기되고, DRP의 롤링 재시작은 즉각 실패하고 있었음. 아래는 그 당시의 몇 가지 증상.

1. 세션 수가 전날보다도 높음.
2. 네트워크 대역폭 사용량이 높았지만 한계치에 도달한 건 아님.
3. 애플리케이션 서버 페이지 레이턴시가 높음.
4. 웹, 애플리케이션, 데이터베이스 CPU 사용량은 매우 낮음.
5. 검색 서버(종종 우리의 범인이었던)는 잘 응답 중. 시스템 통계는 건강해 보임.
6. 요청을 다루는 스레드들이 모두 바쁨. 상당수의 스레드가 5초 이상 자신의 일을 처리하는 중.

사실, 페이지 레이턴시는 문제를 잘 드러내지 못했음. 오래 걸리는 것들은 타임아웃이 걸리고, 통계는 성공적으로 요청을 처리한 경우만을 계산한 결과이기 때문. (그렇다고 실패하는 응답을 포함하기도 어려움. 실패하는 요청은 일반적으로 성공한 경우보다 빠른 시간 안에 끝나기 때문. 잘못된 응답 지연을 만들어 내는 것은 마찬가지. 목적을 생각해 보자) 문제가 있다는 전화를 받은지 거의 90분이 지나고 있었음. SLA를 어기기 직전의 상황.

프론트엔드 애플리케이션 서버의 스레드 덤프는 다음과 같은 모습을 보여주고 있었음.

1. 일부 스레드가 백엔드 호출을 위해 바쁜 상태였고,
2. 대부분의 스레드는 백엔드 연결을 기다리는 중.
3. 백엔드에 대한 연결 리소스 풀에는 타임아웃이 X.
4. 백엔드가 응답을 주지 않으면 프론트엔드는 계속 기다려야만 하는 상황.

다음으로 주문 관리 시스템을 스레드 덤프로 확인.

1. 외부 연동 지점 호출에 450개의 스레드가 사용되고,
2. 나머지 스레드는 이 호출이 끝나기를 기다리는 중.
3. 이 외부 연동 지점은 배송을 위한 스케쥴링 서버.
4. 보통 4대의 인스턴스가 가동되지만,
5. 유지보수를 위해 휴일 동안 2대를 중단시켰으며,
6. 남은 인스턴스 중 1대는 알 수 없는 이유로 이상 동작을 보이고 있었음.

결국, [여기 그림](https://learning.oreilly.com/library/view/release-it-2nd/9781680504552/images/case_study_living_space/scheduling.png)에서 보듯 시스템 크기의 큰 불균형이 발생했던 것. 그런데, 남아 있던 스케쥴링 서버가 1대였으면, 부하가 발생했을 텐데, 왜 모니터링이 되지 않았을까?

1. 알림을 받긴 했지만,
2. 평소에도 일시적인 CPU 스파이크 알림이 있었고,
3. 이 스파이크는 거짓 경보였기에,
4. 서버가 1대만 남아 생긴 부하 알림까지 무시하게 된 것.

여러가지 방법을 찾던 중, 문제가 생기면 커넥션 풀 관리 컴포넌트를 재시작(복구)하기로 함. 구체적으로는 커넥션 관리 컴포넌트의 `max` 프로퍼티를 적절히 설정하고, `stopService`와 `startService`를 호출하는 것. 책에서는 이게 *recovery-oriented computing*의 핵심 개념이라고 함. ROC 제안의 수준은 아니겠지만, 전체를 재시작하는 대신, 일부만 복구하는 것.

참고로, ROC 원칙은 다음과 같음.

1. Failures are inevitable, in both hardware and software.
2. Modeling and analysis can never be sufficiently complete. A *priori* prediction of all failure modes is not possible.
3. Human action is a major source of system failures.

